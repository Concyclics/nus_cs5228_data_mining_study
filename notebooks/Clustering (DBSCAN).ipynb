{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: DBSCAN\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular algorithm used for clustering data points based on their density distribution in a given dataset. It is particularly effective in identifying clusters of arbitrary shape and handling outliers or noise points. The DBSCAN algorithm works as follows:\n",
    "\n",
    "* **Density-Based:** DBSCAN identifies clusters based on the density of data points. It defines density as the number of data points within a specified radius (eps) around each point.\n",
    "\n",
    "* **Core Points:** DBSCAN starts by randomly selecting a data point and checks if there are at least a minimum number of points (min_samples) within a radius of eps around it. If this condition is satisfied, the point is labeled as a \"core point\" and becomes the starting point of a new cluster.\n",
    "\n",
    "* **Directly Density-Reachable:** DBSCAN expands the cluster around a core point by finding other core points that are directly density-reachable from it. A point is considered directly density-reachable if it lies within the eps neighborhood of another core point.\n",
    "\n",
    "* **Density-Connected:** DBSCAN further expands the cluster by iteratively finding points that are density-reachable from the core points, even if they are not core points themselves. This process continues until no more density-reachable points are found.\n",
    "\n",
    "* **Border Points:** If a point is not a core point but lies within the eps neighborhood of a core point, it is labeled as a \"border point\" and may be part of a cluster but does not contribute to the expansion of the cluster.\n",
    "\n",
    "* **Noise Points:** Data points that are neither core points nor border points are labeled as \"noise points\" or outliers, as they do not belong to any cluster.\n",
    "\n",
    "The DBSCAN algorithm does not require specifying the number of clusters in advance, making it more flexible than algorithms like K-means. It can discover clusters of varying shapes and sizes, and it is robust to outliers. However, it does depend on the proper choice of the `eps` and `min_samples` parameters, which can impact the resulting clusters.\n",
    "\n",
    "**Side note:** This notebook includes different evaluation metrics to assess the quality of clusterings, which will be covered a bit later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Specify how Plots Get Rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Toy Data\n",
    "\n",
    "[`sklearn.datasets`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) provides a series of methods to randomly generate sample data. \n",
    "\n",
    "Try different methods and see how the results will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_demo, y_demo = make_blobs(n_samples=100, centers=5, n_features=2, cluster_std=0.85, random_state=11)\n",
    "X_demo = X_demo/10 # only needed for make_blobs to have a range from -1 to 1 as well (otherwise the eps value would differ too much for different datasets)\n",
    "\n",
    "#X_demo, y_demo = make_moons(n_samples=250, noise=0.105, random_state=0)\n",
    "#X_demo, y_demo = make_circles(n_samples=500, noise=0.06, factor=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the data to get a first idea how our data looks like. Of course, in practice this might not be (trivially) possible with data points of more than 3 dimensions. The following example, however, focus on illustrating the characteristics of K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(X_demo[:,0], X_demo[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below plots the clustering, and we will use it throughout the rest of the notebook. The main input of this methods is an instance of [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) with which the clusters have been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(dbscan, data, point_size=50, show_ticks=True, aspect=None):\n",
    "    plt.figure()\n",
    "    \n",
    "    # Optionally, set aspect ration (only need for lat/lng data)\n",
    "    if aspect is not None:\n",
    "        plt.axes().set_aspect(aspect)\n",
    "    \n",
    "    # Get the indices of all the points that belong to a cluster (core or boundary point)\n",
    "    # (the cluster labels go from 0 to (num_clusters-1)\n",
    "    cluster_point_indices  = np.where(dbscan.labels_ >= 0)[0]\n",
    "    \n",
    "    # Get the list of indices of the noise data points (the label of noise is -1)\n",
    "    noise_indices = np.where(dbscan.labels_ < 0)[0]\n",
    "    \n",
    "    # Plot the noise points in black (throws an error if there's no noise)\n",
    "    # The dots of noise are smaller and plotted first, so cluster points will always be more prominent\n",
    "    try:\n",
    "        plt.scatter(data[noise_indices, 0], data[noise_indices, 1], c='black', s=int(point_size/4))    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Plot clusters, each cluster in a different color (at least w.r.t. to the colormap)\n",
    "    plt.scatter(data[cluster_point_indices, 0], data[cluster_point_indices, 1], c=dbscan.labels_[cluster_point_indices], s=point_size, cmap=plt.cm.tab10)\n",
    "\n",
    "    # Optionally, remove all ticks and labels\n",
    "    if show_ticks is False:\n",
    "        plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)  \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "dbscan_demo = DBSCAN(eps=0.1, min_samples=8).fit(X_demo) \n",
    "plot_clusters(dbscan_demo, X_demo, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming, the blob data, for `eps=0.1` and `min_samples=8`, we can see that some of the outer points of the blobs don't make it into the cluster as the density is to low. These are the **noise points**. Try different parameter values and see how the clustering changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Different Parameters\n",
    "\n",
    "With `eps` and `min_samples`, DBSCAN has two core parameters affecting the resulting clustering. So we can use 2 nested loops to vary both parameters over meaningful ranges. For each parameter setting, we perform DBSCAN and keep track of the resulting silhouette score and the (adjusted) rand index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette, ari = [], []\n",
    "\n",
    "for min_samples in np.arange(2, 20, 1):\n",
    "    for eps in np.arange(0.01, 1.0, 0.01):\n",
    "        \n",
    "        # Run DBSCAN for the current parameter values\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(X_demo) \n",
    "\n",
    "        # silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed clusters\n",
    "        try:\n",
    "            silhouette.append((min_samples, eps, silhouette_score(X_demo, dbscan.labels_)))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        ari.append((min_samples, eps, adjusted_rand_score(y_demo, dbscan.labels_)))\n",
    "    \n",
    "# Convert to numpy array for convenience\n",
    "silhouette = np.array(silhouette)    \n",
    "ari = np.array(ari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having two input parameters, we can visualize the results using a 3d plot. Let's first plot the results for the rand index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'min_samples', fontsize=16)\n",
    "ax.set_ylabel(r'eps', fontsize=16)\n",
    "ax.set_zlabel('Rand Index', fontsize=16)\n",
    "ax.view_init(20, 160)\n",
    "surf = ax.plot_trisurf(ari[:,0], ari[:,1], ari[:,2], cmap=plt.cm.coolwarm, linewidth=0, antialiased=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For `make_blobs()` with the default values:** Given the range of the parameters, the value for `eps` has clearly a greater effect on the result compared to `min_samples`. This is due to the blobs reasonably well separated and of similar density. As soon as an increasing `eps` value crosses a threshold, 2 clusters merge into one -- reflected in the step-by-step reduction in the rand index.\n",
    "\n",
    "Now the plot for the silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'min_samples', fontsize=16)\n",
    "ax.set_ylabel(r'eps', fontsize=16)\n",
    "ax.set_zlabel('Silhouette', fontsize=16)\n",
    "ax.view_init(20, 160)\n",
    "surf = ax.plot_trisurf(silhouette[:,0], silhouette[:,1], silhouette[:,2], cmap=plt.cm.coolwarm, linewidth=0, antialiased=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For `make_blobs()` with the default values:** The silhouette score is more stable than the rand index for a range of parameter values, since the calculation of the silhouette score only considers cluster points but NOT noise points. Whether an outer point of a blob is part of the cluster or not, does not make a big difference.\n",
    "\n",
    "By finding the parameter setting with the highest rand index, we can perform DBSCAN with these setting and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = np.argmax(ari[:,2])\n",
    "best_min_samples = int(ari[:,0][best_run])\n",
    "best_eps = ari[:,1][best_run]\n",
    "\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples).fit(X_demo) \n",
    "plot_clusters(dbscan, X_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do the same thing using the silhoutte scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = np.argmax(silhouette[:,2])\n",
    "best_min_samples = int(silhouette[:,0][best_run])\n",
    "best_eps = silhouette[:,1][best_run]\n",
    "\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples).fit(X_demo) \n",
    "plot_clusters(dbscan, X_demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the non-blob datasets, the parameter setting resulting in the highest silhouette score does yield the two intuitively preferred clusters. The main reason is that the silhouette score, similar to SSE, favors blob-like clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN over a Real-World Dataset\n",
    "\n",
    "The [Pima Indians Diabetes Dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) is a well-known dataset frequently used in machine learning and data mining research. It contains information about a group of Pima Indian women from Arizona, USA, and their potential risk of developing diabetes. The dataset is commonly used for classification tasks, aiming to predict whether a person has diabetes or not based on various features.\n",
    "\n",
    "Here are the details of the Pima diabetes dataset:\n",
    "\n",
    "* Number of instances: 768\n",
    "* Number of attributes: 8\n",
    "* Target variable: Outcome (0 for non-diabetic, 1 for diabetic)\n",
    "\n",
    "The attributes or features in the dataset are:\n",
    "\n",
    "* Pregnancies: Number of times pregnant.\n",
    "* Glucose: Plasma glucose concentration after a 2-hour oral glucose tolerance test.\n",
    "* Blood Pressure: Diastolic blood pressure (mm Hg).\n",
    "* Skin Thickness: Triceps skinfold thickness (mm).\n",
    "* Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "* BMI: Body mass index (weight in kg / (height in m)^2).\n",
    "* Diabetes Pedigree Function: Diabetes pedigree function, which provides an estimation of the genetic influence.\n",
    "* Age: Age in years.\n",
    "\n",
    "This is a convenient dataset since all attributes for the clustering are numerical, and we can use the default Euclidean distance as a similarity measure. First, as usual, we read the dataset from the file into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = pd.read_csv('data/datasets/diabetes/diabetes.csv')\n",
    "\n",
    "df_diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Remember, in practice, you should always do at least a basic EDA to check for missing values and (obvious) outliers! We skip this step to keep the notebook simple.\n",
    "\n",
    "First, we need to create the input data `X` for the clustering and -- for this example -- the label data `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diabetes = df_diabetes.drop(columns=['Outcome']).to_numpy()\n",
    "y_diabetes = df_diabetes['Outcome'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can normalize the data, e.g., using standardization. In fact, it is highly recommended that we do so as the domains of different attributes differ by several orders of magnitude. You can check again with the EDA and Data Preprocessing notebook regarding the importance of normalizing input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_diabetes = scaler.fit_transform(X_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Parameter Settings\n",
    "\n",
    "Selecting meaningful parameters for `eps` and `min_samples` is generally not obvious. One basic approach to get some basic sense for meaningful `eps` values is to look at the distribution of pairwise distances between all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn makes it very convenient to calculate pairwise distances\n",
    "dist = euclidean_distances(X_diabetes, X_diabetes)\n",
    "\n",
    "# Matrix dist is symmentrical so we can ignore half of it\n",
    "# (including the diagonal where all values a 0)\n",
    "dist = dist[np.triu_indices_from(dist, k=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a histogram to plot the distribution of pairwise lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.hist(dist, bins=200)\n",
    "plt.xlabel('pairwise distance', fontsize=16)\n",
    "plt.ylabel('count', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot gives us a crude idea how to set `eps`. For example, values (far) above the average are likely to yield a single big cluster; try `eps=4`. On the other hand, values close to 0 are arguably too small to result in too many noise points and very small clusters. Note that these values will also depend on whether the attributes have been standardized or not!!!\n",
    "\n",
    "Let's run DBSCAN with some first crude choices for `eps` and `min_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_diabetes = DBSCAN(eps=1, min_samples=5).fit(X_diabetes) \n",
    "\n",
    "print(np.unique(dbscan_diabetes.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "As our dataset now has more than 3 attributes, we cannot directly plot the clusters. However, we can use dimensionality reduction techniques to map the data points into a lower-dimensional space. In this example, we use PCA to map our data into 2d -- a detailed discussion about PCA is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diabetes_pca = PCA(n_components=2).fit_transform(X_diabetes)\n",
    "X_diabetes_tsne = TSNE(n_components=2, init='random').fit_transform(X_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(dbscan_diabetes, X_diabetes_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(dbscan_diabetes, X_diabetes_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will have noticed, some of the clusters do not look as you would expect and you have seen in the plots above. For example some noise points might be very close to cluster points. The difference is that we performed the clustering by running DBSCAN in the 3d space and then used PCA to map the data into the 2d space. The issue is that dimensionality reduction always results in some loss of information. Here, we lose the separation of clusters in the plot, but only in the plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is particularly interesting for datasets where we already know or expect the data not to be blobs but of different shapes. A good example is location data, where bus stops or bars might cluster along a popular road. Of course, bars or restaurants might also occur in the shape of blobs in popular areas.\n",
    "\n",
    "The following CSV file contains a list of places/venues with their type and geographic location in terms of latitude-longitude pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This CSV file has not header line at the top, so the columns will be unnamed\n",
    "# but if you look at the data you should identify the meaning of each column\n",
    "df = pd.read_csv('data/datasets/singapore/sg-places.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at all possible values for Column 2, which reflects the type of a place, just to see what's available to us. You can of course consider all places, but things get quickly too cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(df[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick all the places of one type. The descriptions in the following well refer to restaurants (in more detail: McDonald's restaurants; see below). But feel free to change the type to anything of your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different place types\n",
    "place_type = 'restaurant'\n",
    "#place_type = 'bus_station'\n",
    "#place_type = 'subway_station' # MRT+LRT stations\n",
    "#place_type = 'store'\n",
    "\n",
    "df_places = df[df[2]==place_type]\n",
    "\n",
    "print('Number of places: {}'.format(df_places.shape[0]))\n",
    "\n",
    "df_places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3k+ restaurants is quite a lot, and maybe even too diverse for an analysis. So let's further filter the dataset to only include McDonald's restaurants. We use regular expressions for that. No worries, if you're not familiar with regular expressions. In a nutshell, they allow for filtering using flexible substring matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'mcdonald', flags=re.IGNORECASE)\n",
    "df_places = df_places[[bool(p.search(x)) for x in df_places[0]]]\n",
    "\n",
    "df_places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_restaurants, num_attributes = df_places.shape\n",
    "\n",
    "print('Number of places: {}'.format(num_restaurants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the column containing latitude and longitude to a matrix, i.e., a 2d numpy array for further processing. Note that the resulting order is \\[longitude, latitude\\], since longitude represents the x variable and latitude the y variable. This doesn't matter for the clustering but it ensures that the plots look alright and are not rotated by 90 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_places = df_places[[4, 3]].to_numpy()\n",
    "\n",
    "print(X_places[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below provides a neat way that the proportions of the plotted points look nicer. Otherwise, the induced shape of Singapore will be squashed. Since Singapore is so close to the equator, this correction is not really needed, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect = 1/np.cos(np.radians(1.35))\n",
    "print(aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the places (e.g., the 141 McDonald's restaurant). You should be able to recognize the outline of Singapore. Of course, if you pick place types that are much less common and/or can only be found in certain areas, you won't be able to \"see\" the outline of Singapore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.axes().set_aspect(aspect)\n",
    "plt.scatter(X_places[:,0], X_places[:,1], s=25)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing DBSCAN\n",
    "\n",
    "By default, the DBSCAN implementation of scikit-learn, would treat the latitude-longitude pairs as coordinates in the Euclidean space. DBSCAN would still work fine, but we would have a hard time to pick intuitive values for `eps`. The Euclidean distance between two geolocations has no meaningful unit such as meter or kilometer.\n",
    "\n",
    "Luckily, DBSCAN allows us to provide a custom similarity/distance metric. The method below calculates the distance between two geolocations in meters; this function can be found on the Web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_distance_in_meters(point1, point2):\n",
    "    lon1, lat1 = point1[0], point1[1]\n",
    "    lon2, lat2 = point2[0], point2[1]\n",
    "    dlon = np.radians(lon2) - np.radians(lon1)\n",
    "    dlat = np.radians(lat2) - np.radians(lat1)\n",
    "\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    distance_haversine_formula = 6371000 * c\n",
    "    return distance_haversine_formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run DBSCAN; notice how the method for calculating distances is given as a parameter. With this, we can now express `eps` in meters, making it much more intuitive to pick suitable values -- note that this is a case where we do not want to normalize/standardize the data.\n",
    "\n",
    "In the example below, we set `eps=500` and `min_samples=5`, which roughly translates to: find clusters with 5 or McDonald's restaurants in a vicinity of 1km (circle with a radius of 500m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_places = DBSCAN(metric=geo_distance_in_meters, eps=500, min_samples=5).fit(X_places) \n",
    "\n",
    "# The set of unique quickly labels shows how many clusters there are; -1 is noise\n",
    "print(np.unique(dbscan_places.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the clusters to see where the McDonald's centers of Singapore are :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(dbscan_places, X_places, show_ticks=False, point_size=50, aspect=aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different Python packages to allow for a fancier plotting of geolocations on top of actual maps. But those are \"only\" visualization details which are beyond this notebook. But note that good visualizations are an important part of data mining to interpret and share results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that identifies clusters in a dataset based on the density distribution of data points. It offers several advantages that make it a popular choice for clustering tasks.\n",
    "\n",
    "One of the main advantages of DBSCAN is its ability to discover clusters of arbitrary shape. Unlike algorithms like k-means, which assume clusters to be spherical, DBSCAN can identify clusters of varying shapes and sizes. This makes it more suitable for real-world datasets where clusters may not conform to strict geometric assumptions.\n",
    "\n",
    "Another advantage is its robustness to outliers and noise. DBSCAN can effectively handle noise points and classify them as outliers, as they do not belong to any cluster. This is particularly useful in scenarios where the dataset may contain incomplete or noisy data.\n",
    "\n",
    "Furthermore, DBSCAN does not require the number of clusters to be predefined. It automatically determines the number of clusters based on the density of the data points. This flexibility makes it suitable for datasets where the number of clusters is unknown or may vary.\n",
    "\n",
    "Despite its advantages, DBSCAN does have some limitations. It requires the appropriate choice of parameters, such as the radius (eps) and the minimum number of points (min_samples) for a point to be considered a core point. Choosing these parameters can be challenging and may impact the clustering results. Additionally, DBSCAN may struggle with datasets of varying densities, as it assumes a uniform density across clusters. If the density varies significantly, it may result in uneven cluster sizes or failure to identify smaller clusters.\n",
    "\n",
    "In summary, DBSCAN is a powerful clustering algorithm that can discover clusters of arbitrary shape, handle outliers, and automatically determine the number of clusters. It is particularly useful for real-world datasets and offers more flexibility compared to traditional clustering algorithms. However, it requires careful parameter selection and may struggle with datasets of varying densities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
