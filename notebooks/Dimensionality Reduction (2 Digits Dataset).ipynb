{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction: Digits Dataset\n",
    "\n",
    "This notebook doesn't use any new concept, but applies PCA, LDA, and t-SNE on different a dataset. The IRIS dataset is \"too simple\": it only has 4 features to begin with, and those 4 features also show strong linear relationships, making it always too easy to apply PCA and LDA and getting good results. Also, the IRIS dataset with only 150 data samples is very small, so all 3 dimensionality reduction methods perform very fast.\n",
    "\n",
    "The [Digits dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html) provided by `scikit-learn` is made up of 1,797 8x8 images. Each image, like the one shown below, is of a hand-written digit, represented as a grayscale image with pixel values ranging from 0 to 16, indicating the intensity of the pixel.. In order to utilize an 8x8 figure like this, we have to first transform it into a feature vector with length 64; this. resulting in 64 features. And since this features represent pixel values, there are no very obvious linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Specify how Plots Get Rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Dataset (Digits)\n",
    "\n",
    "The Digits dataset is part of the `scikit-learn` package, making it very easy to load into the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print('The dataset contains {} sample and {} features.'.format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration, the code cell below show a few images from the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=1, ncols=6, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title('Sample: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "As the dataset is still not really large, we can easily afford to calculate PCA with different values for `n_compinents` and see how much of the variance is explained by all new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals, y_vals = [], []\n",
    "\n",
    "for n in tqdm(range(1, 33)):\n",
    "    \n",
    "    pca = PCA(n_components=n).fit(X)\n",
    "\n",
    "    x_vals.append(n)\n",
    "    y_vals.append(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to observe is that the computation is still very fast. After all, the dataset is not truly large.\n",
    "\n",
    "Note that with a value between 0 and 1 for the overall explained variance for each value of `n_component` we can use a line plot to visualize the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('y', fontsize=18)\n",
    "plt.ylabel('percentage of variance (%)')\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the IRIS dataset, we need quite a number of principal components to explain a good amount of the variance, e.g., more than 20 components to explain more than 90% of the variance. This is in line with our intuition that there are no clear linear relationship between the features representing pixel values.\n",
    "\n",
    "For plotting, this is a bit problematic. Of course, we can reduce the number of features down to 2 and have a look at the resulting plot. But from the result we have so far we can expect a very poor separation of the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X)\n",
    "\n",
    "print('Overall explained variance: {:.3f}'.format(np.sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the corresponding plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=cm.tab10, s=50)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can vaguely make out the 10 different classes, but the overlap between the classes is equally noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "We can do a similar analysis with LDA, but note that the maximum number of components we can try is the number of classes minus 1. So let's first calculate this number -- although we know that it's 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "print('Number of classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same loop to iterate over all possible values of `n_components` for LDA, for each value keeping track of the resulting overall explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals, y_vals = [], []\n",
    "\n",
    "for n in tqdm(range(1, num_classes)):\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(n_components=n).fit(X, y)\n",
    "\n",
    "    x_vals.append(n)\n",
    "    y_vals.append(np.sum(lda.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the computation takes essentially no time, and we can visualize the result using a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xlabel('x', fontsize=18)\n",
    "plt.ylabel('y', fontsize=18)\n",
    "plt.ylabel('percentage of variance (%)')\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the trend is similar to the one exhibited by PCA, the absolute values for the explained variances are larger for LDA. Here, only 6 components explained around 90% of the variance. Still, for 2 components this value is still below 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=2).fit(X, y)\n",
    "\n",
    "print('Overall explained variance: {:.3f}'.format(np.sum(lda.explained_variance_ratio_)))\n",
    "\n",
    "X_lda = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the dataset with the 2 new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap=cm.tab10, s=50)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the plot looks very similar to the one for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Lastly, we use t-SNE to reduce the dimensionality of the dataset. Since we cannot quantify the quality of the reduction like with PCA and LDA using the overall explained variance, we can directly reduce the dimensionality to 2 features. As in the previous notebook, feel free to run the algorithm multiple times with the same parameters settings to see that the results will vary due to the indeterministic nature of t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_tsne = TSNE(n_components=2, perplexity=50).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that t-SNE runs noticeably slower compared to PCA and LDA since t-SNE is an iterative algorithm. For really large datasets, this can be a significant challenge.\n",
    "\n",
    "Anyway, we can plot the dataset using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=cm.tab10, s=25)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that t-SNE runs noticeably slower compared to PCA and LDA since t-SNE is an iterative algorithm. For really large datasets, this can be a significant challenge.\n",
    "\n",
    "Anyway, we can plot the dataset using a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Compared to the IRIS Dataset, PCA and LDA don't yield similar \"clean\" 2d scatter plots like t-SNE since the features do not have strong linear relationships with each other. However, note that visualization is often not the main reason for dimensionality reduction and therefore not a good benchmark to assess the effectiveness of a dimensionality reduction technique.\n",
    "\n",
    "A main challenge with t-SNE is the potentially very long runtime in case of datasets with a (very) large number of samples and features. In practice, a common way to address this is to first apply PCA on the dataset to reduce the number of features \"a bit\" (e.g., by an order of magnitude) and then apply t-SNE on the results. For the Digits dataset here, since it's arguably a small dataset, applying first PCA and then t-SNE shows almost no difference when it comes to the runtimes. However, for the [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database), the differences can be significant, reducing the runtime of t-SNE on the original from ~1h down to ~5min when first applying PCA (with enough components to yield a similar 2d visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
