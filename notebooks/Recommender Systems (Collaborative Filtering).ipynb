{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems: Collaborative Filtering\n",
    "\n",
    "Collaborative filtering recommender systems are a type of recommendation system that predicts a user's preferences or interests by collecting and analyzing information from multiple users. The fundamental idea behind collaborative filtering is that if two users have similar preferences or interests on some items, they are likely to have similar preferences on other items as well.\n",
    "\n",
    "Collaborative filtering algorithms typically work by building a model or matrix of user-item interactions or ratings. This model is then used to make predictions or recommendations for items that a user has not yet interacted with. There are two main approaches to collaborative filtering:\n",
    "\n",
    "* **User-based collaborative filtering:** In this approach, the system identifies users who have similar preferences to the target user and recommends items that those similar users have liked or rated highly. The system looks for users with similar patterns of item ratings and uses their preferences to predict recommendations for the target user.\n",
    "\n",
    "* **Item-based collaborative filtering:** In this approach, the system identifies items that are similar to the ones the target user has liked or interacted with in the past. It then recommends items that are similar to those the user has shown interest in. This method is based on the assumption that if a user likes one item, they are likely to be interested in similar items.\n",
    "\n",
    "Collaborative filtering has been widely used in recommendation systems, particularly in domains such as e-commerce, music, and movie recommendations. It can provide personalized recommendations based on the collective wisdom and behavior of a large user community. However, collaborative filtering approaches can face challenges such as the cold start problem (when there is limited or no information about a new user or item) and the sparsity problem (when the user-item interaction matrix is sparse, meaning there are few ratings or interactions available for most users and items)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Make all Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Prepare MovieLens Dataset (Small)\n",
    "\n",
    "Throughout this notebook, we are using a small sample of [MovieLens](https://grouplens.org/datasets/movielens/) dataset. GroupLens Research has collected and made available rating data sets from the [MovieLens web site](https://movielens.org). Apart from the ratings, the dataset also comes with tags assigned to movies. The information about the movies include their id, title, and genre(s). Here are some key features of the MovieLens dataset:\n",
    "\n",
    "* **Ratings:** The dataset includes ratings given by users to movies on a numerical scale, typically ranging from 1 to 5. Users provide their subjective ratings based on their personal preferences.\n",
    "\n",
    "* **Movie Metadata:** MovieLens dataset also provides additional information about movies, such as genre, release year, and tags. This metadata can be used to build content-based recommendation models.\n",
    "\n",
    "* **User Information:** The dataset contains demographic information about users, such as age, gender, occupation, and zip code. This information can be utilized to analyze user preferences and personalize recommendations.\n",
    "\n",
    "* **Dataset Size:** The MovieLens dataset is available in different sizes. The smallest version, MovieLens 100K, contains 100,000 ratings from approximately 1,000 users on around 1,700 movies. Larger versions, such as MovieLens 1M, MovieLens 10M, and MovieLens 20M, contain respectively 1 million, 10 million, and 20 million ratings.\n",
    "\n",
    "Content-based recommender systems assume that the items (here: movies) come with a set of features describing each item. In this notebook, to keep it simple, we limit ourselves to the genre of movies. We will see later why only using genres does not result in great recommendations. Including the tags would certainly improve this, but would also increase the complexity quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Movies\n",
    "\n",
    "We first load all movies into a `pandas` DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file using pandas\n",
    "df_movies = pd.read_csv('data/datasets/ml-latest-small/movies.csv', sep=',', engine='python')\n",
    "\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that all the genres of a movie are represented as a single string with the genres separated by a pipe symbol. So simplify subsequent steps, let's convert the dataframe into a dictionary with the movie ids as keys and the information about the movies as values. Each value is a 2-tuple with the movie title and the genres (but as set and all genres being lowercase).\n",
    "\n",
    "**Important:** We keep the genres only to add some information to the recommendations; the genres are NOT used to find the recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary {artist_id -> (movie_name, genres)}\n",
    "movie_dict = { row[0]: (row[1], set(row[2].lower().split('|'))) for row in df_movies.values}\n",
    "\n",
    "print(movie_dict[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, when we deal with the User-Item matrix, users and movies are identified by the row and column index in that matrix. For example the movie with id 1000 might be represented by the 700th column in the rating matrix To map back to the movies and users, we have to create mappings that allows us to map between movie and users ids and their respective row/column indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = df_movies['movieId'].unique()\n",
    "\n",
    "movie_id2idx, movie_idx2id = {}, {}\n",
    "\n",
    "for idx, movie_id in tqdm(enumerate(movie_ids), total=len(movie_ids)):\n",
    "    movie_id2idx[movie_id] = idx\n",
    "    movie_idx2id[idx] = movie_id\n",
    "\n",
    "num_movies = len(movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ratings\n",
    "\n",
    "Now we can look at the ratings. Again, we first load the information into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('data/datasets/ml-latest-small/ratings.csv', sep=',', engine='python')\n",
    "\n",
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have to create the mapping between the ids of users and their respective indices in the User-Item matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df_ratings['userId'].unique()\n",
    "\n",
    "user_id2idx, user_idx2id = {}, {}\n",
    " \n",
    "for idx, user_id in tqdm(enumerate(user_ids), total=len(user_ids)):\n",
    "    user_id2idx[user_id] = idx\n",
    "    user_idx2id[idx] = user_id\n",
    "\n",
    "num_users = user_ids.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing both movies and ratings, we can get a sense of the size of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of users: {}'.format(num_users))\n",
    "print('Number of movies: {}'.format(num_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, 610 users and 9,742 can hardly be considered a big dataset. But to get the basic ideas and concepts of content-based recommender systems, it's more than sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize User-Item Matrix\n",
    "\n",
    "The core information of recommender systems is the user-item matrix M. For our use case, the items are the movies and the matrix elements represent the rating of a user about a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.zeros((num_users, num_movies))\n",
    "\n",
    "print('The User-Item Matrix R has shape of {}'.format(R.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, all elements in R are currently 0. Now we can go through ratings in df_ratings to fill. Note how we need to map the user and movie ids to valid matrix indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_ratings.iterrows(), total=len(df_ratings)):\n",
    "    user_id, movie_id, rating = row['userId'], row['movieId'], row['rating']\n",
    "\n",
    "    # Convert movie and user ids to indices \n",
    "    user_idx = user_id2idx[user_id]\n",
    "    movie_idx = movie_id2idx[movie_id]\n",
    "    \n",
    "    # Fill matrix at the right spot with the rating\n",
    "    R[user_idx][movie_idx] = rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how sparse matrix R is, i.e., what percentage of entries are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nonzero = np.count_nonzero(R > 0)\n",
    "percent_sparsity = num_nonzero / np.prod(R.shape) * 100\n",
    "\n",
    "print('Number of non-zero entries in R: {} (sparsity: {:.3f}%)'.format(num_nonzero, percent_sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparsity level of around 1.7% is actually rather high. In real-world settings with many more users and movies, the sparsity is typically much lower than that. This obviously calls for more efficient data structures to store very sparse matrices, but that's beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Average User and Movie Ratings.\n",
    "\n",
    "We saw in the lecture that we need to normalize the ratings w.r.t, to users average user ratings. Firstly, all ratings are positive (1-5). This means that there's no explicit notion of \"dislike\" and unrated movies (0) would be treated as rated worst. And secondly, different users might have different notions of what rating represents a good movie. For example, one user might rate a good movie with 3 or higher, while another user considers good movies only from 4 upwards.\n",
    "\n",
    "The following code cell calculated the average rating for each user as well as the average rating of each movie. Note that we have to exclude unrated movies (rating=0) which would otherwise distort these averages. We can do this by \"masking\" the 0-ratings so they are not considered when computing the averages. The actual normalization of the ratings is done below as it differs between the User-User and the Item-Item approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = np.ma.masked_equal(R, 0)\n",
    "movie_mean_ratings = np.mean(masked, axis=0)\n",
    "user_mean_ratings = np.mean(masked, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Based Collaborative Filtering\n",
    "\n",
    "Memory-based collaborative filtering is a technique used in collaborative filtering recommender systems that relies on the similarity between users or items to make recommendations. It is called \"memory-based\" because it uses the entire dataset of user-item interactions or ratings to calculate similarities and make predictions.\n",
    "\n",
    "In memory-based collaborative filtering, the system builds a user-item matrix that represents the historical interactions or ratings of users for different items. This matrix can be sparse, meaning that most entries are unknown or missing. The system then computes the similarity between users or items based on the available data. Common similarity metrics include cosine similarity and Pearson correlation coefficient.\n",
    "\n",
    "User-based memory-based collaborative filtering predicts a user's preferences by finding other users who have similar tastes and interests. It identifies users with similar patterns of item ratings and uses their ratings to estimate the target user's preferences for unrated items. The system calculates the predicted rating for an item by taking a weighted average of the ratings given by similar users for that item.\n",
    "\n",
    "Item-based memory-based collaborative filtering, on the other hand, focuses on the similarity between items. It identifies items that are similar to the ones the user has interacted with and recommends those similar items. To make predictions, the system calculates the predicted rating for an item by taking a weighted average of the user's ratings for similar items.\n",
    "\n",
    "Memory-based collaborative filtering has several advantages. It is easy to understand and implement, and it can capture complex relationships and user preferences. However, it also has limitations. It can be computationally expensive and may not scale well to large datasets. Additionally, it suffers from the sparsity problem when there are few ratings or interactions available for most users and items.\n",
    "\n",
    "### User-Based\n",
    "\n",
    "The User-based approach considers two users as similar if the rated the same items (here: movies) similarly. The idea is to find users with the same or similar taste and user the ratings to estimate the rating for a movie a user has not yet rated.\n",
    "\n",
    "#### Normalization of Ratings\n",
    "\n",
    "For normalize the ratings by subtracting the average user rating. Note that we do this first for all matrix elements (incl. unknown ratings) and the reset the ratings of unrated movies to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract row mean from ratings (NOTE: this is also applied to zero values!)\n",
    "R_normalized = R - user_mean_ratings\n",
    "\n",
    "# Set the zero fields back to zero\n",
    "R_normalized[R == 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the User Similarities\n",
    "\n",
    "The User-based approach defines a user profile as the vector of normalized ratings. Based on this, we can calculate the similarity between users. With this, 2 users are similar, if (a) they rated a similar set of movies and (b) they rated those movies similarly (indicating a similar taste).\n",
    "\n",
    "With [`cosine_similarity()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) provides a handy method to calculate all similarities for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarities = cosine_similarity(R_normalized, R_normalized)\n",
    "\n",
    "print(user_similarities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the shape of this matrix is `(num_users, num_users)`. The following examples of matrix elements are purely for a very basic sanity check: the similarity between the same user should be 1, and the similarities should be symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_similarities[0,0]) # Should be 1 (apart from floating point issues) \n",
    "\n",
    "print(user_similarities[0,1]) # These two should return the same value\n",
    "print(user_similarities[1,0]) # due to the symmetry of the distance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have prepared all the required data to calculate the estimates of a rating given a user `user_idx` and movie `movie_idx`. The method `calculate_rating_user()` does this by performing the following to main steps:\n",
    "\n",
    "* First, find the most similar users to `user_idx` that have also rated `movie_idx`\n",
    "* Secondly, calculate the rating as the weighted average over the ratings given by the most similar users. The weight is the similarity between the users (i.e., the more similar the higher the weight)\n",
    "\n",
    "The method contains comments to provide further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rating_user(user_idx, movie_idx, k=10):\n",
    "    ##\n",
    "    ## Step 1: find all neighbors = USERS most similar to user_idx that have also rated movie_idx\n",
    "    ##\n",
    "\n",
    "    ## Sort users based on similarity\n",
    "    neighbors = np.argsort(user_similarities[user_idx])[::-1]\n",
    "\n",
    "    ## Remove any neighbor who (a) hasn't rated the movie (b) has a similarity of 0 with user_idx\n",
    "    neighbors = [ n_idx for i, n_idx in enumerate(neighbors) if R[n_idx][movie_idx] != 0 and user_similarities[user_idx][n_idx] > 0]\n",
    "\n",
    "    ## Focus only on the top-k neighbors\n",
    "    topk_neighbor_indices = neighbors[:k]\n",
    "\n",
    "\n",
    "    ##\n",
    "    ## Step 2 calculate the rating as the weighted avarage over the ratings given by the neighbors\n",
    "    ##\n",
    "    \n",
    "    ## Get the similarity values between the user and each neighbor\n",
    "    ## (this values are used as the weights; the more similar the neighbor, the more import his/her rating)\n",
    "    neighbor_similarities = user_similarities[user_idx][topk_neighbor_indices]\n",
    "    \n",
    "    ## Get the ratings the neighbors have given the movie\n",
    "    ## (recall that we consider only neighbors who have indeed rated the movie)\n",
    "    neighbor_ratings = R[topk_neighbor_indices][:,movie_idx]\n",
    "\n",
    "    ## Just a fallback to avoid corner cases (e.g., the movie hasn't been rated by anybody)\n",
    "    if np.sum(neighbor_similarities) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    ## Return the weighted average rating as the predicted rating\n",
    "    return np.round(np.sum(neighbor_similarities * neighbor_ratings) / np.sum(neighbor_similarities), 2)\n",
    " \n",
    "    \n",
    "## Compute the rating for an example user/movie pair\n",
    "print(calculate_rating_user(366, 6783))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Based\n",
    "\n",
    "The Item-based approach considers two items as similar if they are equally rated by users. The idea is to find items that are similar to an unrated movie and use their rating to estimate the unknown ratings.\n",
    "\n",
    "\n",
    "#### Normalization of Ratings\n",
    "\n",
    "First we normalize the ratings by subtracting the average movie rating. Note that we do this first for all matrix elements (incl. unknown ratings) and the reset the ratings of unrated movies to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract row mean from ratings (NOTE: this is also applied to zero values!)\n",
    "R_normalized = R - movie_mean_ratings\n",
    "\n",
    "# Set the zero fields back to zero\n",
    "R_normalized[R == 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Item Similarities\n",
    "\n",
    "The Item-based approach defines an item profile as the vector of normalized ratings. Based on this, we can calculate the similarity between items. With this, 2 items are similar, if (a) they have been rated by a similar set of users and (b) they have been rated similarly.\n",
    "\n",
    "With [`cosine_similarity()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) provides a handy method to calculate all similarities for us. Note that -- compared to the User-User approach -- we have to use the transpose of R to get the right result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_similarities = cosine_similarity(R_normalized.T, R_normalized.T)\n",
    "\n",
    "print(movie_similarities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, the shape of this matrix is `(num_movies, num_movies)`. The following examples of matrix elements are purely for a very basic sanity check: the similarity between the same movies should be 1, and the similarities should be symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_similarities[0,0]) # Should be 1 (apart from floating point issues) \n",
    "print(movie_similarities[0,1]) # These two should return the same value\n",
    "print(movie_similarities[1,0]) # due to the symmetry of the distance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we can now define a method `calculate_rating_item()` calculate the estimate a rating given a user `user_idx` and movie `movie_idx` by performing the following to main steps:\n",
    "\n",
    "* First, find the most similar movies to `movie_idx` that have been rated by `user_idx`\n",
    "* Secondly, calculate the rating as the weighted average over the ratings of the most similar movies. The weight is the similarity between the movies (i.e., the more similar the higher the weight)\n",
    "\n",
    "The method contains comments to provide further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rating_item(user_idx, movie_idx, k=10):\n",
    "    ##\n",
    "    ## Step 1: Find all neighbors = MOVIES most similar to movie_idx that have been rated movie_idx\n",
    "    ##\n",
    "   \n",
    "    ## Sort movies based on similarity\n",
    "    neighbors = np.argsort(movie_similarities[movie_idx])[::-1]\n",
    "    \n",
    "    ## Remove any neighbor who (a) hasn't rated the movie (b) has a similarity of 0 with user_idx\n",
    "    neighbors = [ n_idx for i, n_idx in enumerate(neighbors) if R[user_idx][n_idx] != 0 and movie_similarities[movie_idx][n_idx] > 0]\n",
    "\n",
    "    ## Focus only on the top-k neighbors\n",
    "    topk_neighbor_indices = neighbors[:k]\n",
    "\n",
    "    \n",
    "    ##\n",
    "    ## Step 2 calculate the rating as the weighted avarage over the ratings given by the neighbors\n",
    "    ##\n",
    "\n",
    "    ## Get the similarity values between the user and each neighboring movies\n",
    "    ## (this values are used as the weights; the more similar, the more import the rating)\n",
    "    neighbor_similarities = movie_similarities[movie_idx][topk_neighbor_indices]\n",
    "    \n",
    "    ## Get the ratings  of the neighboring movies for the the movie\n",
    "    ## (recall that we consider only neighboring movies who have indeed rated by the user)\n",
    "    neighbor_ratings = R[:,topk_neighbor_indices][user_idx]\n",
    "    \n",
    "    ## Just a fallback to avoid corner cases (e.g., the user hasn't rated any movie yet)\n",
    "    if np.sum(neighbor_similarities) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    ## Return the weighted average rating as the predicted rating\n",
    "    return np.round(np.sum(neighbor_similarities * neighbor_ratings) / np.sum(neighbor_similarities), 3)    \n",
    "            \n",
    "print(calculate_rating_item(4, 0, k=10))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Rating Matrix\n",
    "\n",
    "With the two methods `calculate_rating_item()` and `calculate_rating_item()` we have now two means to predict an unknown rating for every (user,movie)-pair. The code cell below performs this step; you only have to uncomment your method of choice.\n",
    "\n",
    "On problem is that this will run quire a while. Firstly there are many unknown ratings as R is naturally very sparse. And secondly, the our implementations of both `calculate_rating_item()` and `calculate_rating_item()` are hardly optimize for performance as this is not the focus here. \n",
    "\n",
    "You best run this code only once -- or twice given that we have to mean to predict the ratings -- and later load use the saved results. As a rough estimate `user-user` will run for about 1h; `item-item` will run for about 8h on a decent machine. Of course, you can limit yourself on the `user-user` approach only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'user'\n",
    "#mode = 'item'\n",
    "\n",
    "## Create a copy of the rating matrix R (just so we don't overwrite R)\n",
    "R_predicted = np.copy(R)\n",
    "\n",
    "## Get count of 0 entries in R (i.e., unknown ratings)\n",
    "num_zero = np.count_nonzero(R == 0)\n",
    "\n",
    "print('Number of ratings to predict: {}'.format(num_zero))\n",
    "\n",
    "with tqdm(total=num_zero) as progress_bar:\n",
    "    for user_idx in range(R_predicted.shape[0]):    \n",
    "        for movie_idx in range(R_predicted.shape[1]):\n",
    "\n",
    "            # Ignore known ratings\n",
    "            if R_predicted[user_idx][movie_idx] > 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate the predicted rating for the current user and movie (given their indices)\n",
    "            if mode == 'user-user':\n",
    "                R_predicted[user_idx][movie_idx] = calculate_rating_user(user_idx, movie_idx, k=25)   \n",
    "            elif mode == 'item-item':\n",
    "                R_predicted[user_idx][movie_idx] = calculate_rating_item(user_idx, movie_idx, k=25)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "            \n",
    "# Save matrix (as the dataset doesn't change, computing the matrix once is enough)     \n",
    "if mode == 'user':\n",
    "    with open('data/datasets/ml-latest-small/cf-user-matrix-movielens-small.npy', 'wb') as f:\n",
    "        np.save(f, R_predicted)\n",
    "elif mode == 'item':\n",
    "    with open('data/datasets/ml-latest-small/cf-item-matrix-movielens-small.npy', 'wb') as f:\n",
    "        np.save(f, R_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Recommendations\n",
    "\n",
    "We first load the matrix containing all ratings incl. the estimates we computed using the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/datasets/ml-latest-small/cf-user-matrix-movielens-small.npy', 'rb') as f:\n",
    "#with open('data/datasets/ml-latest-small/cf-item-matrix-movielens-small.npy', 'rb') as f:\n",
    "    R_predicted = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have a complete rating matrix, making recommendations is pretty straightforward. We define the method `get_recommendation_cf()` for this. The basic steps are explained in the method and should be rather easy to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_cf(R_predicted, user_id, topk=10, factor=2, remove_rated=True):\n",
    "    \n",
    "    ## Get the user index given the user id\n",
    "    user_idx = user_id2idx[user_id]\n",
    "    \n",
    "    ## Start with ranking all movies based on their (estimated) rankings\n",
    "    recommendations = np.argsort(R_predicted[user_idx].squeeze())\n",
    "    \n",
    "    ## Remove movies from recommendations the use has only rated\n",
    "    if remove_rated == True:\n",
    "        already_rated_movies = np.where(R[user_idx] != 0)[0]\n",
    "        \n",
    "        recommendations = np.delete(recommendations, already_rated_movies)\n",
    "        \n",
    "    ## Focus on only on the top topk*factor recommendations\n",
    "    recommendations = recommendations[::-1][:topk*factor]\n",
    "    \n",
    "    ## Pick a random topk sample of topk*factor recommendations\n",
    "    recommendations = np.random.choice(recommendations, size=topk, replace=False)\n",
    "    \n",
    "    ## Sort the recommendations w.r.t. the average movie rating from best to worst\n",
    "    ## (not really needed)\n",
    "    #recommendations = sorted(recommendations, key=lambda tup: tup[1], reverse=True) \n",
    "    \n",
    "    ## Return the indices (sorted) for all recommended movies\n",
    "    return np.array([ movie_idx2id[r] for r in recommendations ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example by recommending movies to the user with `user_id=1`. Feel free to pick a different user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "\n",
    "for rank, movie_id in enumerate(get_recommendation_cf(R_predicted, user_id)):\n",
    "    \n",
    "    ## Convert the user and movie id to their respective indices\n",
    "    user_idx, movie_idx = user_id2idx[user_id], movie_id2idx[movie_id]\n",
    "    \n",
    "    ## Get the title and genres of the recommended movies\n",
    "    title, genres = movie_dict[movie_id]\n",
    "    \n",
    "    ## Get the average rating of the movie\n",
    "    avg_rating = movie_mean_ratings[movie_idx]\n",
    "    \n",
    "    ## Get the estimated rating for the movie be the user\n",
    "    pred_rating = R_predicted[user_idx,movie_idx]\n",
    "    \n",
    "    ## Print the results nicely\n",
    "    print('[Rank {} ({:.2f}/{:.2f})] {} {}'.format(rank+1, pred_rating, avg_rating, title, '/'.join(genres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the genres between the recommendations can differ quite a lot. This is no surprise as we do not consider the genres while making the recommendations. Thus, a nice extension would be to organize the recommendations regarding the genre to make it easier for the user to browse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based: Matrix Factorization\n",
    "\n",
    "Model-based recommender systems are a type of recommendation system that employs machine learning algorithms to build predictive models based on the available user-item data. These models capture the underlying patterns, relationships, and preferences within the data to make recommendations for users. Unlike memory-based collaborative filtering that directly uses the raw user-item interactions or ratings, model-based approaches involve a training phase where the system learns from the data to create a model. This model can then be used to make predictions for new users and items.\n",
    "\n",
    "Model-based recommender systems typically employ techniques such as matrix factorization, clustering, or classification algorithms to learn the latent features and relationships in the data. Some commonly used algorithms include:\n",
    "\n",
    "* **Matrix Factorization:** Matrix factorization methods decompose the user-item interaction matrix into lower-dimensional matrices to capture latent features. Popular matrix factorization techniques include Singular Value Decomposition (SVD) and its variations like Probabilistic Matrix Factorization (PMF) and Non-Negative Matrix Factorization (NMF).\n",
    "\n",
    "* **Bayesian Networks:** Bayesian networks model the relationships between variables and use probabilistic inference to make predictions. They can capture dependencies and correlations between items or users to provide personalized recommendations.\n",
    "\n",
    "* **Neural Networks:** Deep learning techniques, such as neural networks, can be used to learn complex patterns and representations from the user-item data. They have the ability to model non-linear relationships and capture intricate user preferences.\n",
    "\n",
    "Model-based recommender systems offer several advantages. They can handle sparsity better than memory-based approaches and can handle large-scale datasets more efficiently. Additionally, they can incorporate additional features, such as item attributes or user demographics, into the models to enhance recommendation accuracy. However, model-based approaches also have some challenges. They require a training phase that involves significant computational resources and time. Moreover, they may suffer from overfitting if the model is too complex or if the dataset is small or noisy.\n",
    "\n",
    "In this notebook -- in line with the lecture -- we look at the approach of Matrix Factorization to make recommendations. More specifically, we consider [Non-Negative Matrix Factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) (NMF or NNMF) since all of our ratings are positive. Non-negative matrix factorization (NMF) is often considered easier compared to general matrix factorization due to its unique properties and constraints. Here are a few reasons why NMF is comparatively easier:\n",
    "\n",
    "* **Non-negativity constraint:** In NMF, both the user-item interaction matrix and the factor matrices are constrained to be non-negative. This non-negativity constraint allows for a more intuitive and interpretable factorization. It can be particularly useful in applications where the values represent non-negative quantities, such as ratings, counts, or probabilities. Non-negativity helps in capturing additive combinations of factors, which can often result in meaningful and sparse representations.\n",
    "\n",
    "* **Simplicity of interpretation:** The non-negativity constraint in NMF provides a natural way to interpret the factor matrices. The factors can be seen as non-negative components or parts that contribute to the overall observation. For example, in a movie recommendation system, the factors can correspond to genre preferences, and the factor values indicate the importance or relevance of each genre for a particular user.\n",
    "\n",
    "* **Reduced dimensionality:** NMF typically aims to factorize a high-dimensional matrix into a lower-dimensional representation. This dimensionality reduction property can be advantageous in terms of computational efficiency and memory usage. By representing the original matrix with a reduced number of factors, NMF can simplify the subsequent computations and improve scalability.\n",
    "\n",
    "* **Sparsity and interpretability:** NMF tends to produce sparse factor matrices, where most entries are zero or close to zero. This sparsity arises naturally from the non-negativity constraint and can be beneficial in recommendation systems. Sparse factor matrices are easier to interpret and can help identify the most influential factors or features driving the recommendations.\n",
    "\n",
    "* **Algorithms and optimization:** NMF has specialized algorithms that are tailored for non-negativity constraints, such as multiplicative updates and alternating least squares. These algorithms are designed to ensure non-negativity during the factorization process, which simplifies the optimization task compared to general matrix factorization methods.\n",
    "\n",
    "Despite its advantages, it is important to note that NMF also has its limitations. For example, the non-negativity constraint may not be suitable for all types of data, and it may not capture complex relationships as effectively as more flexible factorization techniques. The choice between NMF and general matrix factorization depends on the specific characteristics of the data and the requirements of the recommendation problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF with Toy Dataset\n",
    "\n",
    "To better see the results of Matrix Factorization by actually printing and comparing the matrices, we create a very small raings matrix $R$ containing just 4 users and 7 items (e.g.: movies). The matrix elements are values from 1, 2, ..., 5 representing the users' ratings of a movie. As usual, 0 means that a user has not rated that movie.\n",
    "\n",
    "#### Create Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_toy = np.array([\n",
    "    [4, 0, 0, 5, 1, 0, 0],\n",
    "    [5, 5, 4, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 2, 4, 5, 0],\n",
    "    [0, 3, 0, 0, 0, 0, 3]\n",
    "], dtype=float)\n",
    "\n",
    "print(R_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform NMF\n",
    "\n",
    "Recall that we want to find 2 matrices $W$ and $H$ such that\n",
    "\n",
    "$$R = W\\ast H$$\n",
    "\n",
    "where $W$ is of shape `(num_users, k)` containing the latent representation of the users, and $H$ is of shape `(k, num_movies)` containing all the latent representations of the movies. $k$ specifies the size of the latent representations. Both $W$ and $H$ are initialized randomly, and then later refined using Gradient Descent or similar methods.\n",
    "\n",
    "The NMF implementation in `scikit-learn` follows the standard NMF formulation, where a given matrix is decomposed into two non-negative matrices: the basis matrix (also known as the $W$ matrix) and the coefficient matrix (also known as the $H$ matrix). The goal is to approximate the input matrix by the product of these two matrices, where the elements are non-negative. `scikit-learn`'s NMF implementation supports different optimization algorithms, such as alternating least squares (ALS) and multiplicative updates -- which generally works much better than basic Gradient Descent. You can specify the algorithm to use by setting the `solver` parameter when creating the NMF instance.\n",
    "\n",
    "The code cell below uses `scikit-learn` to perform NMF with latent representations of size `k=100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "\n",
    "nmf = NMF(n_components=k, init='random', random_state=0)\n",
    "\n",
    "W = nmf.fit_transform(R_toy)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have look as the how the model performs by checking the dot product of W and H which predicts the rating matrix $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_toy_predicted = np.around(np.dot(W, H), decimals=2)\n",
    "\n",
    "print(R_toy_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for this simple toy data, the predicted ratings are almost identical to the true ratings.\n",
    "\n",
    "### NMF with MovieLens Dataset\n",
    "\n",
    "Now we can perform NMF to built a recommender system for our MovieLens dataset. In the code cell below, we again use `scikit-leans`'s implementation of NMF to compute the matrices $W$ and $H$. With the given parameters, you should see good results, but feel free to change them and observe their effects on the results.\n",
    "\n",
    "**Side note:** Compared to the toy dataset, the code cell below will take some seconds/minutes to run. To see some feedback, we set `verbose=1` to print the *violation* after each epoch. Note that the violation is NOT the objective function. It is the sum of the absolute value of the projected gradient, and it is used only as a stopping criterion. The objective function is costly to compute, so it is not computed at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k = 500\n",
    "\n",
    "nmf = NMF(n_components=k, init='random', random_state=0, max_iter=20, verbose=1)\n",
    "\n",
    "W = nmf.fit_transform(R)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $W$ and $H$ being computed, we can compute the matrix with all predicted ratings again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_predicted = np.around(np.dot(W, H), decimals=2)\n",
    "\n",
    "with open('data/datasets/ml-latest-small/cf-nmf-movielens-small.npy', 'wb') as f:\n",
    "    np.save(f, R_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rating matrices are too large, let's just look and compare a couple of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R[0][:20])\n",
    "print(R_predicted[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for the toy dataset, the true ratings and predicted ratings should be again very similar (assuming the default values for the input parameters)\n",
    "\n",
    "\n",
    "### Making Recommendations\n",
    "\n",
    "We first load the matrix containing all ratings incl. the estimates we computed using the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/datasets/ml-latest-small/cf-nmf-movielens-small.npy', 'rb') as f:\n",
    "    R_predicted = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the matrix of predicted ratings, we can now make movie recommendations the same as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "\n",
    "for rank, movie_id in enumerate(get_recommendation_cf(R_predicted, user_id)):\n",
    "    \n",
    "    ## Convert the user and movie id to their respective indices\n",
    "    user_idx, movie_idx = user_id2idx[user_id], movie_id2idx[movie_id]\n",
    "    \n",
    "    ## Get the title and genres of the recommended movies\n",
    "    title, genres = movie_dict[movie_id]\n",
    "    \n",
    "    ## Get the average rating of the movie\n",
    "    avg_rating = movie_mean_ratings[movie_idx]\n",
    "    \n",
    "    ## Get the estimated rating for the movie be the user\n",
    "    pred_rating = R_predicted[user_idx,movie_idx]\n",
    "    \n",
    "    ## Print the results nicely\n",
    "    print('[Rank {} ({:.2f}/{:.2f})] {} {}'.format(rank+1, pred_rating, avg_rating, title, '/'.join(genres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we show the estimates ratings just to show that they don't have to be with the range of 1-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "A recommender system based on collaborative filtering is a type of information filtering system that predicts users' preferences and interests by collecting and analyzing their behavior and feedback, as well as the behavior and feedback of similar users. Collaborative filtering techniques leverage the collective intelligence of a community to provide personalized recommendations to individuals. There are two main approaches to collaborative filtering: memory-based recommender systems and model-based recommender systems.\n",
    "\n",
    "Memory-based recommender systems use the actual data from the user-item interactions to make recommendations. These systems typically employ two types of collaborative filtering: user-based and item-based. User-based collaborative filtering recommends items to a user based on the interests and preferences of similar users. It finds users who have similar patterns of item ratings or purchases and suggests items that those similar users have liked. Item-based collaborative filtering, on the other hand, recommends items to a user based on the similarities between the items themselves. It identifies items that are similar to the ones the user has rated positively and suggests those similar items.\n",
    "\n",
    "Model-based recommender systems, also known as algorithmic or latent factor models, build a mathematical model from the user-item interactions to generate recommendations. These systems use techniques like matrix factorization or singular value decomposition to identify latent factors or hidden features that capture the underlying patterns and correlations in the data. By learning these latent factors, model-based recommender systems can make predictions and provide recommendations based on the relationships between users and items. These models are trained on large datasets and can handle sparse and noisy data, making them more scalable and robust compared to memory-based approaches.\n",
    "\n",
    "Both memory-based and model-based recommender systems have their advantages and limitations. Memory-based methods are simple and intuitive, easy to implement, and can handle new users and items without retraining the model. However, they suffer from scalability issues as the dataset grows, and they struggle with the sparsity of data. Model-based approaches, on the other hand, offer better scalability and can provide more accurate recommendations, especially for sparse data. However, they require a training phase and are more complex to implement and maintain.\n",
    "\n",
    "In summary, collaborative filtering-based recommender systems leverage user behavior and feedback to make personalized recommendations. Memory-based systems directly analyze user-item interactions, while model-based systems use mathematical models to capture underlying patterns and make predictions. Both approaches have their strengths and weaknesses, and the choice between them depends on factors such as dataset size, sparsity, scalability requirements, and implementation complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
