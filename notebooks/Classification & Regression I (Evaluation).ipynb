{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification & Regression I: Evaluation Metrics\n",
    "\n",
    "Classification and regression are methods for predicting either class labels (classification) or numerical values (regression). Both methods work by find patterns between the input (sets of attribute/features) and the output labels or output values. This requires a dataset of many example of (input, output)-pairs from which to derive meaningful patterns.\n",
    "\n",
    "In practice, having such a dataset is often the bigger challenge compared to applying mature classification and regression models. On the upside, existing (input, output)-pairs allows to evaluate models as those pairs provide ground truth to compare predictions to. \n",
    "\n",
    "Comparing the numerical output and predicted values in case of regression is typically a straightforward task using error function such as the (Root) Mean Squared Error (R)MSE. Less obvious is the comparison between ground truth and prediction in case of classifications. In this notebook, we look a bit closer into the evaluation of classification models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Specify how Plots Get Rendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make all Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Toy Data\n",
    "\n",
    "We first start with some toy data containing 10 predictions (`y_pred`) of a binary classification task and the corresponding 10 ground truth labels (`y_true`). In this example, we assume that the predictions are not (directly) class labels 0 or 1 but numerical values reflecting the probability of Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([0.95, 0.3, 0.4, 0.55, 0.1, 0.45, 0.9, 0.6, 0.4, 0.65])\n",
    "y_true = np.array([1, 0, 0, 0, 1, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on a `threshold` we can map the probabilities into class labels. Since we only have two classes (i.e., binary classification) we can assign Class 1 to any prediction with a probability greater or equal to `threshold`; Class 0 otherwise. That's easy to do with [np.qhere](https://numpy.org/doc/stable/reference/generated/numpy.where.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "\n",
    "y_pred_classes = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "print('y_true = {}'.format(y_true))\n",
    "print('y_pred = {}'.format(y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provide a handy method to evaluate the [`confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) with matrix element $A_{ij}$ representing the number of samples of Class $i$ predicted to Class $j$.\n",
    "\n",
    "In case of binary classification, the confusion matrix is a 2x2 matrix and allows to directly extract the number of true positives (TP), false positives (FP), false negative (FN), and true negatives (TN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true, y_pred_classes))\n",
    "\n",
    "# ravel() \"flattens\" the 2x2 matrix to directly have access to all 4 elements\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred_classes).ravel()\n",
    "\n",
    "print('TP: {}\\nFP: {}\\nFN: {}\\nTN: {}'.format(tp, fp, fn, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visually confirm the correctness of the results. For example, `TP: 3` means that there are 3 samples where both `y_pred` and `y_true` have a 1 at the same position in the arrays. You can change the `threshold` and see its effect on the confusion matrix.\n",
    "\n",
    "Another useful method provided by `scitkit-learn` is [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) that prints a basic summary of the evaluation results. This summary contains the precision, recall, and f1-score for each class (i.e., 1-vs-rest) as well as the averages over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC)\n",
    "\n",
    "We have seen above that the choice of `threshold` can greatly affect the evaluation results. The common way to visualize the effect the choice of `threshold` on the results is by means of the Receiver Operating Characteristic (ROC) curve which plots the False Positive Rate (1-Specificity) against the True Positivity Rate (Sensitivity) for different thresholds.\n",
    "\n",
    "Of course, `scitkit-learn` comes with a respective method [`roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) that returns all FPRs and TPRs together with the respective threshold for a pair of prediction and ground truth labels. Note that this method also calculates all meaningful thresholds -- that is, the threshold that actually affects the FPR and TPR values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need the thresholds for the plot\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is a useful visualization tool but does not provide simple numerical values that would allow to compare different curves in a straightforward manner. To this end, we introduced the Area Under the Curve for the Receiver Operating Characteristic (AUC-ROC). Again, `scitkit-learn` has us covered with the method [`roc_auc_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those values, we can plot the ROC curve as a basic line plot. The dashed line represents the ROC of a random classifier. We define a method for plotting as we can re-use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel(\"False Positive Rate (1-Specificity)\", fontsize=16)\n",
    "    plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize=16)\n",
    "    line = plt.plot(fpr, tpr, marker='o', lw=3, markersize=10, label='ROC curve (area = {:.2f})'.format(roc_auc))[0]\n",
    "    line.set_clip_on(False)\n",
    "    plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC from Scratch\n",
    "\n",
    "The method [`roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) is the go-to tool in practice. However, to really understand the idea behind the ROC curve, it's best to implement it from scratch to really see what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sensitivity and Specificity\n",
    "\n",
    "scikit-learn does not provide methods to directly get the sensitivity and specificity for a classifier. However, we can easily calculate both values based on the TP/FP/FN/TN values we get from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_values(y_true, y_pred):\n",
    "    # Get the individual tp/fp/fn/tn values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    # Calculate sensitivity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    # Return both values\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the method for different thresholds and see how the sensitivity and specificity changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "y_pred_classes = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "sensitivity, specificity = calc_roc_values(y_true, y_pred_classes)\n",
    "\n",
    "print('Sensitivity: {}\\nSpecificity: {}'.format(sensitivity, specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ROC¶\n",
    "\n",
    "Calculating the ROC involves two steps:\n",
    "\n",
    "* Calculate all meaningful thresholds (meaningful = will affect the results)\n",
    "* Calculate false-positive rate and true-positive rate for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_roc(y_true, y_pred):\n",
    "    \n",
    "    #############################################################################\n",
    "    ## Calculate all \"meaningful\" thresholds\n",
    "    #############################################################################\n",
    "    \n",
    "    # Get unique predicted values (result is conveniently sorted)\n",
    "    values = np.unique(y_pred)\n",
    "    \n",
    "    # Add 0 at the beginning and 1.0 at the end\n",
    "    values = np.insert(values, 0, 0.0)\n",
    "    values = np.insert(values, values.shape[0], 1.0)\n",
    "    \n",
    "    # Calculate thresholds as the average of all subsequent value pairs\n",
    "    # Only the value between two sample values make meaningful thresholds\n",
    "    thresholds = np.array([(values[i]-values[i-1])/2 + values[i-1] for i, _ in enumerate(values) if i > 0])\n",
    "    \n",
    "    #############################################################################\n",
    "    ## Calculate false-positive rate and true-positive rate for each threshold\n",
    "    #############################################################################\n",
    "    fpr, tpr = [], []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        # Convert probilities to class labels\n",
    "        y_pred_classes = np.where(y_pred >= t, 1, 0)\n",
    "        # Calculate sensitivity and specificity\n",
    "        sensitivity, specificity = calc_roc_values(y_true, y_pred_classes)\n",
    "        # Keep track of false positive and true positive rates\n",
    "        fpr.append(1-specificity)\n",
    "        tpr.append(sensitivity)\n",
    "        \n",
    "    # Return arrays with false-postive rates and true-positive rates\n",
    "    return fpr, tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `eval_roc()` the same way as we did `roc_curve()`; note that `eval_roc()` does not return the used threshold as we don't really need them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = eval_roc(y_true, y_pred)\n",
    "\n",
    "# We use the scikit-learn method for calculating the AUC-ROC\n",
    "roc_auc = roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the overall shape of both ROC curves is the same. Note, however, that `eval_roc()` returns 1 more FPR/TPR value at `(0.2, 0.4)`. This is because we didn't optimize the set of meaningful thresholds, but used a simple heuristic. This does not change the results but of course can affect the efficiency as more threshold than needed are checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Data: Diabetes Dataset\n",
    "\n",
    "We are now ready to look at some real-world dataset, but still simple one to help understanding. The [Pima Indians Diabetes Dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) is a well-known dataset frequently used in machine learning and data mining research. It contains information about a group of Pima Indian women from Arizona, USA, and their potential risk of developing diabetes. The dataset is commonly used for classification tasks, aiming to predict whether a person has diabetes or not based on various features.\n",
    "\n",
    "Here are the details of the Pima diabetes dataset:\n",
    "\n",
    "* Number of instances: 768\n",
    "* Number of attributes: 8\n",
    "* Target variable: Outcome (0 for non-diabetic, 1 for diabetic)\n",
    "\n",
    "The attributes or features in the dataset are:\n",
    "\n",
    "* Pregnancies: Number of times pregnant.\n",
    "* Glucose: Plasma glucose concentration after a 2-hour oral glucose tolerance test.\n",
    "* Blood Pressure: Diastolic blood pressure (mm Hg).\n",
    "* Skin Thickness: Triceps skinfold thickness (mm).\n",
    "* Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "* BMI: Body mass index (weight in kg / (height in m)^2).\n",
    "* Diabetes Pedigree Function: Diabetes pedigree function, which provides an estimation of the genetic influence.\n",
    "* Age: Age in years.\n",
    "\n",
    "This is a convenient dataset since all attributes for the clustering are numerical, and we can use the default Euclidean distance as a similarity measure. First, as usual, we read the dataset from the file into a `pandas` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = pd.read_csv('data/datasets/diabetes/diabetes.csv')\n",
    "\n",
    "# The rows are sorted, so let's shuffle them\n",
    "df_diabetes = df_diabetes.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Show the first 5 columns\n",
    "df_diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Data\n",
    "\n",
    "We use a common 80/20 split to create the training the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df_diabetes[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']].to_numpy()\n",
    "y = df_diabetes[['Outcome']].to_numpy().squeeze()\n",
    "\n",
    "# Let's go for a 80%/20% split -- you can change the value anf see its effects\n",
    "train_test_ratio = 0.80\n",
    "\n",
    "# Calculate the size of the training data (the size of the dest data is also implicitly given)\n",
    "train_set_size = int(train_test_ratio * len(X))\n",
    "\n",
    "# Split data and labels into training and test data with respect to the size of the test data\n",
    "X_train, X_test = X[:train_set_size], X[train_set_size:]\n",
    "y_train, y_test = y[:train_set_size].squeeze(), y[train_set_size:].squeeze()\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have only numerical values as input attributes there magnitudes and ranges differ noticeable. It's therefore a good idea to normalize/standardize the data. Feel free to skip this step and compare the results.\n",
    "\n",
    "As usual, `scitkit-learn` makes it very convenient by providing a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) (among other methods for normalization and standardization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the scaler based on the training data only\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Of course, we need to convert both training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Classifier Using Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use, off-the-shelf implementations of different classifiers, the training is extremely simple. Not that we consider classifiers here that (optionally) return probabilities and not just class labels. The only reason for this is to plot the ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "#clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "#clf = DecisionTreeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For the evaluation, we can use all the methods we introduced above for our toy dataset.\n",
    "\n",
    "#### Classes/Labels\n",
    "\n",
    "The method `predict()` directly returns class labels. All classifiers provide this method. This allows us to directly print the confusion matrix and the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the method `classification_report()` to print the f1 scores in a pretty fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilities\n",
    "\n",
    "The method `predict_proba()` returns the probabilities of class labels. Only classifiers that by their definition return probabilities offer this method. Not that each entry returns the probabilities for each class. For example, for binary classification, each entry comes with 2 probabilities. Naturally, all probabilities for each entry sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "# As we have a binary task, we can focus on the probability for Class 1.\n",
    "y_pred = y_pred[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now everything in place to plot the ROC curve and calculate the AUC-ROC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "plot_roc_curve(fpr, tpr, roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Data: IRIS Dataset\n",
    "\n",
    "We use the [IRIS dataset](https://archive.ics.uci.edu/ml/datasets/iris) as a small but real-world dataset to illustrate the use of a KNN classifier. While it is smaller than the Diabetes dataset with less features, it comes with 3 classes, so we can look how to handle cases beyond binary classification tasks.. The Iris dataset is one of the most well-known and commonly used datasets in machine learning for classification tasks. It is named after the iris flower and was introduced by the statistician and biologist Ronald Fisher in 1936. The dataset is often used as a beginner's dataset for learning classification algorithms.\n",
    "\n",
    "The Iris dataset consists of measurements of four features (attributes) of iris flowers, namely:\n",
    "\n",
    "* Sepal Length (in centimeters)\n",
    "* Sepal Width (in centimeters)\n",
    "* Petal Length (in centimeters)\n",
    "* Petal Width (in centimeters)\n",
    "\n",
    "Based on these features, the dataset assigns each instance (row) to one of three classes of iris flowers: *Setosa*, *Versicolor*, and *Virginica*. The dataset contains 150 instances in total, with 50 instances per class. It is a balanced dataset, meaning that each class has an equal number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/datasets/iris/iris.csv')\n",
    "\n",
    "# Convert the species name to numerical categories 0, 1, 2\n",
    "df['species'] = pd.factorize(df['species'])[0]\n",
    "\n",
    "# The rows are sorted, so let's shuffle them\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Show the first 5 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training and Test Data\n",
    "\n",
    "Same as above, we split the dataset into training and test data. Note that we introduce some noise in terms of additional features with random noise. Otherwise, this dataset is too \"clean\" and most classifiers will work almost perfectly. Feel free to remove the adding of the noise and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to numpy arrays\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].to_numpy()\n",
    "y = df[['species']].to_numpy()\n",
    "\n",
    "print('The original dataset has {} samples, each with {} features'.format(X.shape[0], X.shape[1]))\n",
    "\n",
    "# The classification task over the original IRIS dataset is too easy, most classifiers perform perfectly\n",
    "# We therefore add noise in terms of additional 50 features with random values\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.concatenate((X, random_state.randn(n_samples, 50 * n_features)), axis=1)\n",
    "\n",
    "print('The modified dataset has {} samples, each with {} features'.format(X.shape[0], X.shape[1]))\n",
    "\n",
    "# Let's go for a 80%/20% split -- you can change the value anf see its effects\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training data (the size of the dest data is also implicitly given)\n",
    "train_set_size = int(train_test_ratio * len(X))\n",
    "\n",
    "# Split data and labels into training and test data with respect to the size of the test data\n",
    "X_train, X_test = X[:train_set_size], X[train_set_size:]\n",
    "y_train, y_test = y[:train_set_size].squeeze(), y[train_set_size:].squeeze()\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(X_train)))\n",
    "print(\"Size of test: {}\".format(len(X_test)))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to standardize the data because of the noise we added. The original data has only features that all represent lengths in cm and are of comparable magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the scaler based on the training data only\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Of course, we need to convert both training and test data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Classifier\n",
    "\n",
    "For multiclass classification task there are two approaches to train a classifier:\n",
    "\n",
    "* Most classifier implementations directly support multiclass datasets. Here the training \"looks\" the same as for the case of binary classification\n",
    "\n",
    "* [`OneVsRestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) explicitly implement the 1-vs-rest training strategy for the given type of classifier.\n",
    "\n",
    "Try both methods and compare the differences -- which should not be that large, in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "#clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "#clf = OneVsRestClassifier(RandomForestClassifier()).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "#### Labels/classes\n",
    "\n",
    "The method `predict()` directly returns class labels. All classifiers provide this method. This allows us to directly print the confusion matrix and the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the method `classification_report()` to print the f1 scores in a pretty fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilities\n",
    "\n",
    "The method `predict_proba()` returns the probabilities of class labels. Only classifiers that by their definition return probabilities offer this method. Not that each entry returns the probabilities for each class. For example, for binary classification, each entry comes with 2 probabilities. Naturally, all probabilities for each entry sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 3 classes, for plotting the ROC curves, we have to use the 1-vs-rest approach. For each class, we consider only the respective probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_rates, tp_rates, roc_auc_scores = [], [], []\n",
    "\n",
    "for c in [0, 1, 2]:\n",
    "    # Get the probabilities for the current class\n",
    "    y_pred_one = y_pred[:,c]\n",
    "    # Convert to probabilities to class labels (as it's not 1-vs-rest, we only have the labels 0 and 1 now!!!)\n",
    "    y_test_one = np.where(y_test==c, 1 , 0)\n",
    "\n",
    "    # Calculate the false positive and true positive rates, as well as the AUC-ROC values\n",
    "    fpr, tpr, _ = roc_curve(y_test_one, y_pred_one)\n",
    "    roc_auc = roc_auc_score(y_test_one, y_pred_one)\n",
    "    \n",
    "    # Keep track for the results for each class\n",
    "    fp_rates.append(fpr)\n",
    "    tp_rates.append(tpr)\n",
    "    roc_auc_scores.append(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 3 ROC curves, so we cannot re-use the `plot_roc_curve()` here. The basic implementation is the same, apart from the 3 line plots now. (Well, we could generate 3 different plots, one for each curve, but having them in the same plot makes it easier to compare the different classes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"False Positive Rate (1-Specificity)\", fontsize=16)\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize=16)\n",
    "\n",
    "for c in [0, 1, 2]:\n",
    "    line = plt.plot(fp_rates[c], tp_rates[c], marker='o', lw=3, markersize=10, label='Class {} ROC curve (area = {:.2f})'.format(c, roc_auc_scores[c]))[0]\n",
    "    line.set_clip_on(False)\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, different classes are easier or more difficult to predict; the details depend on the random noise added to the data. If you remove the noise (see above) the result will be much better, almost perfect -- and a bit boring for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The availability of ground truth information in the context of classification and regression tasks enables the evaluation of classification and regression models. In this notebook, we looked a the most common techniques to evaluate classifiers -- since the evaluation of regressors is more straightforward.\n",
    "\n",
    "Evaluation metrics are essential for assessing the performance of machine learning classifiers and understanding how well they generalize to unseen data. Here is a brief summary of common evaluation metrics used for machine learning classifiers:\n",
    "\n",
    "* **Accuracy:** Accuracy measures the overall correctness of the classifier's predictions. It calculates the ratio of correctly classified instances to the total number of instances in the test set. While accuracy is a widely used metric, it may not be suitable for imbalanced datasets where one class dominates the others. In such cases, additional evaluation metrics are needed.\n",
    "\n",
    "* **Precision, Recall, and F1 Score:** Precision represents the proportion of correctly predicted positive instances out of all instances predicted as positive. Recall (also known as sensitivity or true positive rate) measures the proportion of correctly predicted positive instances out of all actual positive instances. The F1 score combines precision and recall into a single metric, providing a balanced evaluation measure. These metrics are particularly useful when dealing with imbalanced datasets, where correctly identifying positive instances is of high importance.\n",
    "\n",
    "* **ROC Curve and AUC:** Receiver Operating Characteristic (ROC) curves plot the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds. They illustrate the trade-off between sensitivity and specificity. The area under the ROC curve (AUC) represents the overall performance of the classifier across different thresholds. Higher AUC values indicate better classifier performance, with an AUC of 1 representing a perfect classifier.\n",
    "\n",
    "By considering these evaluation metrics, practitioners can assess the strengths and weaknesses of machine learning classifiers. It is important to choose metrics that align with the specific goals and requirements of the problem at hand. Evaluating classifiers using appropriate metrics helps in selecting the most suitable model, fine-tuning parameters, and improving the overall performance of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5246",
   "language": "python",
   "name": "cs5246"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
